{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending pandas Series"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this exercise, you'll load sales data from the months January, February, and March into DataFrames. Then, you'll extract Series with the 'Units' column from each and append them together with method chaining using .append().\n",
    "\n",
    "To check that the stacking worked, you'll print slices from these Series, and finally, you'll add the result to figure out the total units sold in the first quarter."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Read the files 'sales-jan-2015.csv', 'sales-feb-2015.csv' and 'sales-mar-2015.csv' into the DataFrames jan, feb, and mar respectively.\n",
    "Use parse_dates=True and index_col='Date'.\n",
    "Extract the 'Units' column of jan, feb, and mar to create the Series jan_units, feb_units, and mar_units respectively.\n",
    "Construct the Series quarter1 by appending feb_units to jan_units and then appending mar_units to the result. Use chained calls to the .append() method to do this.\n",
    "Verify that quarter1 has the individual Series stacked vertically. To do this:\n",
    "Print the slice containing rows from jan 27, 2015 to feb 2, 2015.\n",
    "Print the slice containing rows from feb 26, 2015 to mar 7, 2015.\n",
    "Compute and print the total number of units sold from the Series quarter1. This has been done for you, so hit 'Submit Answer' to see the result!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv('sales-jan-2015.csv', parse_dates=True, index_col='Date')\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv('sales-feb-2015.csv', parse_dates=True, index_col='Date')\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv('sales-mar-2015.csv', parse_dates=True, index_col='Date')\n",
    "\n",
    "# Extract the 'Units' column from jan: jan_units\n",
    "jan_units = jan['Units']\n",
    "\n",
    "# Extract the 'Units' column from feb: feb_units\n",
    "feb_units = feb['Units']\n",
    "\n",
    "# Extract the 'Units' column from mar: mar_units\n",
    "mar_units = mar['Units']\n",
    "\n",
    "# Append feb_units and then mar_units to jan_units: quarter1\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64\n",
      "642\n"
     ]
    }
   ],
   "source": [
    "# Print the first slice from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "\n",
    "# Print the second slice from quarter1\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
    "\n",
    "# Compute & print total sales in quarter1\n",
    "print(quarter1.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating pandas Series along row axis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Having learned how to append Series, you'll now learn how to achieve the same result by concatenating Series instead. You'll continue to work with the sales data you've seen previously. This time, the DataFrames jan, feb, and mar have been pre-loaded.\n",
    "\n",
    "Your job is to use pd.concat() with a list of Series to achieve the same result that you would get by chaining calls to .append().\n",
    "\n",
    "You may be wondering about the difference between pd.concat() and pandas' .append() method. One way to think of the difference is that .append() is a specific case of a concatenation, while pd.concat() gives you more flexibility, as you'll see in later exercises."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Create an empty list called units. This has been done for you.\n",
    "Use a for loop to iterate over [jan, feb, mar]:\n",
    "In each iteration of the loop, append the 'Units' column of each DataFrame to units.\n",
    "Concatenate the Series contained in the list units into a longer Series called quarter1 using pd.concat().\n",
    "Specify the keyword argument axis='rows' to stack the Series vertically.\n",
    "Verify that quarter1 has the individual Series stacked vertically by printing slices. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list: units\n",
    "units = []\n",
    "\n",
    "# Build the list of Series\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month['Units'])\n",
    "\n",
    "# Concatenate the list: quarter1\n",
    "quarter1 = pd.concat(units, axis='rows')\n",
    "\n",
    "# Print slices from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending DataFrames with ignore_index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this exercise, you'll use the Baby Names Dataset (from data.gov) again. This time, both DataFrames names_1981 and names_1881 are loaded without specifying an Index column (so the default Indexes for both are RangeIndexes).\n",
    "\n",
    "You'll use the DataFrame .append() method to make a DataFrame combined_names. To distinguish rows from the original two DataFrames, you'll add a 'year' column to each with the year (1881 or 1981 in this case). In addition, you'll specify ignore_index=True so that the index values are not used along the concatenation axis. The resulting axis will instead be labeled 0, 1, ..., n-1, which is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Create a 'year' column in the DataFrames names_1881 and names_1981, with values of 1881 and 1981 respectively. Recall that assigning a scalar value to a DataFrame column broadcasts that value throughout.\n",
    "Create a new DataFrame called combined_names by appending the rows of names_1981 underneath the rows of names_1881. Specify the keyword argument ignore_index=True to make a new RangeIndex of unique integers for each row.\n",
    "Print the shapes of all three DataFrames. This has been done for you.\n",
    "Extract all rows from combined_names that have the name 'Morgan'. To do this, use the .loc[] accessor with an appropriate filter. The relevant column of combined_names here is 'name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = [\"Name\", \"Gender\", \"Frequency\"]\n",
    "names_1881 = pd.read_csv(\"names1881.csv\", header=None)\n",
    "names_1981 = pd.read_csv(\"names1981.csv\", header=None)\n",
    "names_1981.columns = columns_name\n",
    "names_1881.columns = columns_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'year' column to names_1881 and names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19455, 4)\n",
      "(1935, 4)\n",
      "(21390, 4)\n"
     ]
    }
   ],
   "source": [
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name Gender  Frequency  year\n",
      "1283   Morgan      M         23  1881\n",
      "2096   Morgan      F       1769  1981\n",
      "14390  Morgan      M        766  1981\n"
     ]
    }
   ],
   "source": [
    "# Print all rows that contain the name 'Morgan'\n",
    "print(combined_names[combined_names['Name'] == 'Morgan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating pandas DataFrames along column axis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The function pd.concat() can concatenate DataFrames horizontally as well as vertically (vertical is the default). To make the DataFrames stack horizontally, you have to specify the keyword argument axis=1 or axis='columns'.\n",
    "\n",
    "In this exercise, you'll use weather data with maximum and mean daily temperatures sampled at different rates (quarterly versus monthly). You'll concatenate the rows of both and see that, where rows are missing in the coarser DataFrame, null values are inserted in the concatenated DataFrame. This corresponds to an outer join (which you will explore in more detail in later exercises).\n",
    "\n",
    "The files 'quarterly_max_temp.csv' and 'monthly_mean_temp.csv' have been pre-loaded into the DataFrames weather_max and weather_mean respectively, and pandas has been imported as pd."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "Create a new DataFrame called weather by concatenating the DataFrames weather_max and weather_mean horizontally.\n",
    "Pass the DataFrames to pd.concat() as a list and specify the keyword argument axis=1 to stack them horizontally.\n",
    "Print the new DataFrame weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TMAX  TAVG\n",
      "0       53  44.0\n",
      "1       44  36.0\n",
      "2       53  49.0\n",
      "3       45  42.0\n",
      "4       43  36.0\n",
      "5       35  30.0\n",
      "6       37  33.0\n",
      "7       34  32.0\n",
      "8       34  31.0\n",
      "9       43  37.0\n",
      "10      38  34.0\n",
      "11      37  33.0\n",
      "12      59  46.0\n",
      "13      62  48.0\n",
      "14      33  29.0\n",
      "15      30  27.0\n",
      "16      29  22.0\n",
      "17      15  11.0\n",
      "18      29  16.0\n",
      "19      29  24.0\n",
      "20      18  11.0\n",
      "21      27  13.0\n",
      "22      28  17.0\n",
      "23      23  14.0\n",
      "24      27  21.0\n",
      "25      29  23.0\n",
      "26      18  12.0\n",
      "27      22  12.0\n",
      "28      36  19.0\n",
      "29      39  32.0\n",
      "...    ...   ...\n",
      "3987    59  44.0\n",
      "3988    59  51.0\n",
      "3989    50  43.0\n",
      "3990    54  42.0\n",
      "3991    58  41.0\n",
      "3992    55  42.0\n",
      "3993    45  43.0\n",
      "3994    48  40.0\n",
      "3995    63  51.0\n",
      "3996    61  49.0\n",
      "3997    63  52.0\n",
      "3998    57  52.0\n",
      "3999    61  49.0\n",
      "4000    64  60.0\n",
      "4001    48  45.0\n",
      "4002    54  45.0\n",
      "4003    53  50.0\n",
      "4004    41  37.0\n",
      "4005    43  35.0\n",
      "4006    55  41.0\n",
      "4007    60  55.0\n",
      "4008    58  53.0\n",
      "4009    69  61.0\n",
      "4010    64  57.0\n",
      "4011    58  52.0\n",
      "4012    61  51.0\n",
      "4013    44  40.0\n",
      "4014    40  33.0\n",
      "4015    35  30.0\n",
      "4016    50  39.0\n",
      "\n",
      "[4017 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# read the weather.csv file\n",
    "weather = pd.read_csv('weather.csv')\n",
    "weather = weather[['STATION', 'DATE', 'TAVG', 'TMIN', 'TMAX']]\n",
    "weather_max = weather['TMAX']\n",
    "weather_mean = weather['TAVG']\n",
    "\n",
    "# Concatenate weather_max and weather_mean horizontally: weather\n",
    "weather = pd.concat([weather_max, weather_mean], axis=1)\n",
    "\n",
    "# Print weather\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading multiple files to build a DataFrame"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is often convenient to build a large DataFrame by parsing many files as DataFrames and concatenating them all at once. You'll do this here with three files, but, in principle, this approach can be used to combine data from dozens or hundreds of files.\n",
    "\n",
    "Here, you'll work with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "pandas has been imported as pd and two lists have been pre-loaded: An empty list called medals, and medal_types, which contains the strings 'bronze', 'silver', and 'gold'."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Iterate over medal_types in the for loop.\n",
    "Inside the for loop:\n",
    "Create file_name using string interpolation with the loop variable medal. This has been done for you. The expression \"%s_top5.csv\" % medal evaluates as a string with the value of medal replacing %s in the format string.\n",
    "Create the list of column names called columns. This has been done for you.\n",
    "Read file_name into a DataFrame called medal_df. Specify the keyword arguments header=0, index_col='Country', and names=columns to get the correct row and column Indexes.\n",
    "Append medal_df to medals using the list .append() method.\n",
    "Concatenate the list of DataFrames medals horizontally (using axis='columns') to create a single DataFrame called medals. Print it in its entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Gold  Silver  Bronze\n",
      "France             NaN   461.0   475.0\n",
      "Germany          407.0     NaN   454.0\n",
      "Italy            460.0   394.0     NaN\n",
      "Soviet Union     838.0   627.0   584.0\n",
      "United Kingdom   498.0   591.0   505.0\n",
      "United States   2088.0  1195.0  1052.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amrit Kumar Gupta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medal_types = ['Gold', 'Silver', 'Bronze']\n",
    "medals = []\n",
    "for medal in medal_types:\n",
    "\n",
    "    # Create the file name: file_name\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    \n",
    "    # Create list of column names: columns\n",
    "    columns = ['Country', medal]\n",
    "    \n",
    "    # Read file_name into a DataFrame: df\n",
    "    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
    "\n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "\n",
    "# Concatenate medals horizontally: medals\n",
    "medals = pd.concat(medals, axis='columns')\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating vertically to get MultiIndexed rows"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When stacking a sequence of DataFrames vertically, it is sometimes desirable to construct a MultiIndex to indicate the DataFrame from which each row originated. This can be done by specifying the keys parameter in the call to pd.concat(), which generates a hierarchical index with the labels from keys as the outermost index label. So you don't have to rename the columns of each DataFrame as you load it. Instead, only the Index column needs to be specified.\n",
    "\n",
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset. Once again, pandas has been imported as pd and two lists have been pre-loaded: An empty list called medals, and medal_types, which contains the strings 'bronze', 'silver', and 'gold'."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Within the for loop:\n",
    "Read file_name into a DataFrame called medal_df. Specify the index to be 'Country'.\n",
    "Append medal_df to medals.\n",
    "Concatenate the list of DataFrames medals into a single DataFrame called medals. Be sure to use the keyword argument keys=['bronze', 'silver', 'gold'] to create a vertically stacked DataFrame with a MultiIndex.\n",
    "Print the new DataFrame medals. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Total\n",
      "       Country               \n",
      "bronze United States   2088.0\n",
      "       Soviet Union     838.0\n",
      "       United Kingdom   498.0\n",
      "       Italy            460.0\n",
      "       Germany          407.0\n",
      "silver United States   1195.0\n",
      "       Soviet Union     627.0\n",
      "       United Kingdom   591.0\n",
      "       France           461.0\n",
      "       Italy            394.0\n",
      "gold   United States   1052.0\n",
      "       Soviet Union     584.0\n",
      "       United Kingdom   505.0\n",
      "       France           475.0\n",
      "       Germany          454.0\n"
     ]
    }
   ],
   "source": [
    "medals = []\n",
    "for medal in medal_types:\n",
    "\n",
    "    file_name = \"%s_top5.csv\" %medal\n",
    "\n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, index_col='Country')\n",
    "    \n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "\n",
    "# Concatenate medals: medals\n",
    "medals = pd.concat(medals, keys=['bronze', 'silver', 'gold'])\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing MultiIndexed DataFrames"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This exercise picks up where the last ended (again using The Guardian's Olympic medal dataset).\n",
    "\n",
    "You are provided with the MultiIndexed DataFrame as produced at the end of the preceding exercise. Your task is to sort the DataFrame and to use the pd.IndexSlice to extract specific slices. Check out this exercise from Manipulating DataFrames with pandas to refresh your memory on how to deal with MultiIndexed DataFrames.\n",
    "\n",
    "pandas has been imported for you as pd and the DataFrame medals is already in your namespace."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Create a new DataFrame medals_sorted with the entries of medals sorted. Use .sort_index(level=0) to ensure the Index is sorted suitably.\n",
    "Print the number of bronze medals won by Germany and all of the silver medal data. This has been done for you.\n",
    "Create an alias for pd.IndexSlice called idx. A slicer pd.IndexSlice is required when slicing on the inner level of a MultiIndex.\n",
    "Slice all the data on medals won by the United Kingdom. To do this, use the .loc[] accessor with idx[:,'United Kingdom'], :."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total    407.0\n",
      "Name: (bronze, Germany), dtype: float64\n",
      "                 Total\n",
      "Country               \n",
      "France           461.0\n",
      "Italy            394.0\n",
      "Soviet Union     627.0\n",
      "United Kingdom   591.0\n",
      "United States   1195.0\n",
      "                       Total\n",
      "       Country              \n",
      "bronze United Kingdom  498.0\n",
      "gold   United Kingdom  505.0\n",
      "silver United Kingdom  591.0\n"
     ]
    }
   ],
   "source": [
    "# Sort the entries of medals\n",
    "medals_sorted = medals.sort_index(level=0)\n",
    "\n",
    "# Print the number of Bronze medals won by Germany\n",
    "print(medals_sorted.loc[('bronze','Germany')])\n",
    "\n",
    "# Print data about silver medals\n",
    "print(medals_sorted.loc['silver'])\n",
    "\n",
    "# Create alias for pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Print all the data on medals won by the United Kingdom\n",
    "print(medals_sorted.loc[idx[:,'United Kingdom'], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating horizontally to get MultiIndexed columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is also possible to construct a DataFrame with hierarchically indexed columns. For this exercise, you'll start with pandas imported and a list of three DataFrames called dataframes. All three DataFrames contain 'Company', 'Product', and 'Units' columns with a 'Date' column as the index pertaining to sales transactions during the month of February, 2015. The first DataFrame describes Hardware transactions, the second describes Software transactions, and the third, Service transactions.\n",
    "\n",
    "Your task is to concatenate the DataFrames horizontally and to create a MultiIndex on the columns. From there, you can summarize the resulting DataFrame and slice some information from it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Construct a new DataFrame february with MultiIndexed columns by concatenating the list dataframes.\n",
    "Use axis=1 to stack the DataFrames horizontally and the keyword argument keys=['Hardware', 'Software', 'Service'] to construct a hierarchical Index from each DataFrame.\n",
    "Print summary information from the new DataFrame february using the .info() method. This has been done for you.\n",
    "Create an alias called idx for pd.IndexSlice.\n",
    "Extract a slice called slice_2_8 from february (using .loc[] & idx) that comprises rows between Feb. 2, 2015 to Feb. 8, 2015 from columns under 'Company'.\n",
    "Print the slice_2_8. This has been done for you, so hit 'Submit Answer' to see the sliced data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 4 columns):\n",
      "Date       20 non-null object\n",
      "Company    20 non-null object\n",
      "Product    20 non-null object\n",
      "Units      20 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 720.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "february = pd.read_csv(\"sales-feb-2015.csv\")\n",
    "\n",
    "# Print february.info()\n",
    "print(february.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "february.head()\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "february = february.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You're now going to revisit the sales data you worked with earlier in the chapter. Three DataFrames jan, feb, and mar have been pre-loaded for you. Your task is to aggregate the sum of all sales over the 'Company' column into a single DataFrame. You'll do this by constructing a dictionary of these DataFrames and then concatenating them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Create a list called month_list consisting of the tuples ('january', jan), ('february', feb), and ('march', mar).\n",
    "Create an empty dictionary called month_dict.\n",
    "Inside the for loop:\n",
    "Group month_data by 'Company' and use .sum() to aggregate.\n",
    "Construct a new DataFrame called sales by concatenating the DataFrames stored in month_dict.\n",
    "Create an alias for pd.IndexSlice and print all sales by 'Mediacore'. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Units\n",
      "         Company               \n",
      "february Acme Coporation     34\n",
      "         Hooli               30\n",
      "         Initech             30\n",
      "         Mediacore           45\n",
      "         Streeplex           37\n",
      "january  Acme Coporation     76\n",
      "         Hooli               70\n",
      "         Initech             37\n",
      "         Mediacore           15\n",
      "         Streeplex           50\n",
      "march    Acme Coporation      5\n",
      "         Hooli               37\n",
      "         Initech             68\n",
      "         Mediacore           68\n",
      "         Streeplex           40\n"
     ]
    }
   ],
   "source": [
    "# Make the list of tuples: month_list\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = {}\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby('Company').sum()\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Units\n",
      "         Company         \n",
      "february Mediacore     45\n",
      "january  Mediacore     15\n",
      "march    Mediacore     68\n"
     ]
    }
   ],
   "source": [
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "The DataFrames bronze, silver, and gold have been pre-loaded for you.\n",
    "\n",
    "Your task is to compute an inner join."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Construct a list of DataFrames called medal_list with entries bronze, silver, and gold.\n",
    "Concatenate medal_list horizontally with an inner join to create medals.\n",
    "Use the keyword argument keys=['bronze', 'silver', 'gold'] to yield suitable hierarchical indexing.\n",
    "Use axis=1 to get horizontal concatenation.\n",
    "Use join='inner' to keep only rows that share common index labels.\n",
    "Print the new DataFrame medals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bronze                                silver                         \\\n",
      "       NOC                Country   Total    NOC                Country   \n",
      "0      USA          United States  1052.0    USA          United States   \n",
      "1      URS           Soviet Union   584.0    URS           Soviet Union   \n",
      "2      GBR         United Kingdom   505.0    GBR         United Kingdom   \n",
      "3      FRA                 France   475.0    FRA                 France   \n",
      "4      GER                Germany   454.0    GER                Germany   \n",
      "5      AUS              Australia   413.0    AUS              Australia   \n",
      "6      ITA                  Italy   374.0    ITA                  Italy   \n",
      "7      HUN                Hungary   345.0    HUN                Hungary   \n",
      "8      SWE                 Sweden   325.0    SWE                 Sweden   \n",
      "9      NED            Netherlands   320.0    NED            Netherlands   \n",
      "10     ROU                Romania   282.0    ROU                Romania   \n",
      "11     JPN                  Japan   270.0    JPN                  Japan   \n",
      "12     RUS                 Russia   240.0    RUS                 Russia   \n",
      "13     CAN                 Canada   227.0    CAN                 Canada   \n",
      "14     GDR           East Germany   225.0    GDR           East Germany   \n",
      "15     POL                 Poland   223.0    POL                 Poland   \n",
      "16     FIN                Finland   209.0    FIN                Finland   \n",
      "17     CHN                  China   193.0    CHN                  China   \n",
      "18     FRG           West Germany   180.0    FRG           West Germany   \n",
      "19     BRA                 Brazil   174.0    BRA                 Brazil   \n",
      "20     DEN                Denmark   152.0    DEN                Denmark   \n",
      "21     BEL                Belgium   150.0    BEL                Belgium   \n",
      "22     NOR                 Norway   144.0    NOR                 Norway   \n",
      "23     SUI            Switzerland   138.0    SUI            Switzerland   \n",
      "24     BUL               Bulgaria   136.0    BUL               Bulgaria   \n",
      "25     KOR           Korea, South   135.0    KOR           Korea, South   \n",
      "26     YUG             Yugoslavia   118.0    YUG             Yugoslavia   \n",
      "27     CUB                   Cuba   109.0    CUB                   Cuba   \n",
      "28     TCH         Czechoslovakia   105.0    TCH         Czechoslovakia   \n",
      "29     ESP                  Spain    92.0    ESP                  Spain   \n",
      "..     ...                    ...     ...    ...                    ...   \n",
      "108    BAR               Barbados     1.0    BAR               Barbados   \n",
      "109    BER               Bermuda*     1.0    BER               Bermuda*   \n",
      "110    DJI               Djibouti     1.0    DJI               Djibouti   \n",
      "111    ERI                Eritrea     1.0    ERI                Eritrea   \n",
      "112    GUY                 Guyana     1.0    GUY                 Guyana   \n",
      "113    IRQ                   Iraq     1.0    IRQ                   Iraq   \n",
      "114    KUW                 Kuwait     1.0    KUW                 Kuwait   \n",
      "115    MKD              Macedonia     1.0    MKD              Macedonia   \n",
      "116    MRI              Mauritius     1.0    MRI              Mauritius   \n",
      "117    NIG                  Niger     1.0    NIG                  Niger   \n",
      "118    TOG                   Togo     1.0    TOG                   Togo   \n",
      "119    PAR               Paraguay     NaN    PAR               Paraguay   \n",
      "120    PER                   Peru     NaN    PER                   Peru   \n",
      "121    SCG                 Serbia     NaN    SCG                 Serbia   \n",
      "122    NAM                Namibia     NaN    NAM                Namibia   \n",
      "123    SIN              Singapore     NaN    SIN              Singapore   \n",
      "124    HKG             Hong Kong*     NaN    HKG             Hong Kong*   \n",
      "125    SRI              Sri Lanka     NaN    SRI              Sri Lanka   \n",
      "126    TAN               Tanzania     NaN    TAN               Tanzania   \n",
      "127    VIE                Vietnam     NaN    VIE                Vietnam   \n",
      "128    ECU                Ecuador     NaN    ECU                Ecuador   \n",
      "129    LUX             Luxembourg     NaN    LUX             Luxembourg   \n",
      "130    AHO  Netherlands Antilles*     NaN    AHO  Netherlands Antilles*   \n",
      "131    CIV          Cote d'Ivoire     NaN    CIV          Cote d'Ivoire   \n",
      "132    ISV        Virgin Islands*     NaN    ISV        Virgin Islands*   \n",
      "133    SEN                Senegal     NaN    SEN                Senegal   \n",
      "134    SUD                  Sudan     NaN    SUD                  Sudan   \n",
      "135    TGA                  Tonga     NaN    TGA                  Tonga   \n",
      "136    BDI                Burundi     NaN    BDI                Burundi   \n",
      "137    UAE   United Arab Emirates     NaN    UAE   United Arab Emirates   \n",
      "\n",
      "            gold                                 \n",
      "      Total  NOC                Country   Total  \n",
      "0    1195.0  USA          United States  2088.0  \n",
      "1     627.0  URS           Soviet Union   838.0  \n",
      "2     591.0  GBR         United Kingdom   498.0  \n",
      "3     461.0  FRA                 France   378.0  \n",
      "4     350.0  GER                Germany   407.0  \n",
      "5     369.0  AUS              Australia   293.0  \n",
      "6     394.0  ITA                  Italy   460.0  \n",
      "7     308.0  HUN                Hungary   400.0  \n",
      "8     349.0  SWE                 Sweden   347.0  \n",
      "9     250.0  NED            Netherlands   212.0  \n",
      "10    187.0  ROU                Romania   155.0  \n",
      "11    228.0  JPN                  Japan   206.0  \n",
      "12    206.0  RUS                 Russia   192.0  \n",
      "13    211.0  CAN                 Canada   154.0  \n",
      "14    271.0  GDR           East Germany   329.0  \n",
      "15    173.0  POL                 Poland   103.0  \n",
      "16    118.0  FIN                Finland   124.0  \n",
      "17    252.0  CHN                  China   234.0  \n",
      "18    167.0  FRG           West Germany   143.0  \n",
      "19    139.0  BRA                 Brazil    59.0  \n",
      "20    192.0  DEN                Denmark   147.0  \n",
      "21    167.0  BEL                Belgium    91.0  \n",
      "22    199.0  NOR                 Norway   194.0  \n",
      "23    165.0  SUI            Switzerland    73.0  \n",
      "24    142.0  BUL               Bulgaria    53.0  \n",
      "25    191.0  KOR           Korea, South   140.0  \n",
      "26    174.0  YUG             Yugoslavia   143.0  \n",
      "27    126.0  CUB                   Cuba   160.0  \n",
      "28    144.0  TCH         Czechoslovakia    80.0  \n",
      "29    193.0  ESP                  Spain    92.0  \n",
      "..      ...  ...                    ...     ...  \n",
      "108     NaN  BAR               Barbados     NaN  \n",
      "109     NaN  BER               Bermuda*     NaN  \n",
      "110     NaN  DJI               Djibouti     NaN  \n",
      "111     NaN  ERI                Eritrea     NaN  \n",
      "112     NaN  GUY                 Guyana     NaN  \n",
      "113     NaN  IRQ                   Iraq     NaN  \n",
      "114     NaN  KUW                 Kuwait     NaN  \n",
      "115     NaN  MKD              Macedonia     NaN  \n",
      "116     NaN  MRI              Mauritius     NaN  \n",
      "117     NaN  NIG                  Niger     NaN  \n",
      "118     NaN  TOG                   Togo     NaN  \n",
      "119    17.0  PAR               Paraguay     NaN  \n",
      "120    14.0  PER                   Peru     1.0  \n",
      "121    14.0  SCG                 Serbia     NaN  \n",
      "122     4.0  NAM                Namibia     NaN  \n",
      "123     4.0  SIN              Singapore     NaN  \n",
      "124     2.0  HKG             Hong Kong*     1.0  \n",
      "125     2.0  SRI              Sri Lanka     NaN  \n",
      "126     2.0  TAN               Tanzania     NaN  \n",
      "127     2.0  VIE                Vietnam     NaN  \n",
      "128     1.0  ECU                Ecuador     1.0  \n",
      "129     1.0  LUX             Luxembourg     1.0  \n",
      "130     1.0  AHO  Netherlands Antilles*     NaN  \n",
      "131     1.0  CIV          Cote d'Ivoire     NaN  \n",
      "132     1.0  ISV        Virgin Islands*     NaN  \n",
      "133     1.0  SEN                Senegal     NaN  \n",
      "134     1.0  SUD                  Sudan     NaN  \n",
      "135     1.0  TGA                  Tonga     NaN  \n",
      "136     NaN  BDI                Burundi     1.0  \n",
      "137     NaN  UAE   United Arab Emirates     1.0  \n",
      "\n",
      "[138 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "gold = pd.read_csv(\"Gold.csv\")\n",
    "silver = pd.read_csv(\"Silver.csv\")\n",
    "bronze = pd.read_csv(\"Bronze.csv\")\n",
    "\n",
    "\n",
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze, silver, gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, keys=['bronze', 'silver', 'gold'], axis=1, join='inner')\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling & concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this exercise, you'll compare the historical 10-year GDP (Gross Domestic Product) growth in the US and in China. The data for the US starts in 1947 and is recorded quarterly; by contrast, the data for China starts in 1966 and is recorded annually.\n",
    "\n",
    "You'll need to use a combination of resampling and an inner join to align the index labels. You'll need an appropriate offset alias for resampling, and the method .resample() must be chained with some kind of aggregation method (.pct_change() and .last() in this case).\n",
    "\n",
    "pandas has been imported as pd, and the DataFrames china and us have been pre-loaded, with the output of china.head() and us.head() printed in the IPython Shell."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INSTRUCTIONS\n",
    "\n",
    "Make a new DataFrame china_annual by resampling the DataFrame china with .resample('A') (i.e., with annual frequency) and chaining two method calls:\n",
    "Chain .pct_change(10) as an aggregation method to compute the percentage change with an offset of ten years.\n",
    "Chain .dropna() to eliminate rows containing null values.\n",
    "Make a new DataFrame us_annual by resampling the DataFrame us exactly as you resampled china.\n",
    "Concatenate china_annual and us_annual to construct a DataFrame called gdp. Use join='inner' to perform an inner join and use axis=1 to concatenate horizontally.\n",
    "Print the result of resampling gdp every decade (i.e., using .resample('10A')) and aggregating with the method .last(). This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "china = pd.read_csv(\"gdp_china.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample and tidy china: china_annual\n",
    "china_annual = china.resample('A').pct_change(10).dropna()\n",
    "\n",
    "# Resample and tidy us: us_annual\n",
    "us_annual = us.resample('A').pct_change(10).dropna()\n",
    "\n",
    "# Concatenate china_annual and us_annual: gdp\n",
    "gdp = pd.concat([china_annual, us_annual], join='inner', axis=1)\n",
    "\n",
    "# Resample gdp and print\n",
    "print(gdp.resample('10A').last())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
